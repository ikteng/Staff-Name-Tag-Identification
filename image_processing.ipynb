{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "b10a1574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3f89e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_rotated_bbox(image, center_x, center_y, width, height, rotation_deg):\n",
    "    \"\"\"\n",
    "    Crop a rotated bounding box from an image\n",
    "    \"\"\"\n",
    "    # Compute rotation matrix\n",
    "    rot_mat = cv2.getRotationMatrix2D((center_x, center_y), rotation_deg, 1.0)\n",
    "    \n",
    "    # Rotate the whole image\n",
    "    rotated = cv2.warpAffine(image, rot_mat, (image.shape[1], image.shape[0]), flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    # Compute coordinates of the bbox in the rotated image\n",
    "    x_min = int(center_x - width / 2)\n",
    "    x_max = int(center_x + width / 2)\n",
    "    y_min = int(center_y - height / 2)\n",
    "    y_max = int(center_y + height / 2)\n",
    "    \n",
    "    # Ensure bounds\n",
    "    x_min, y_min = max(x_min, 0), max(y_min, 0)\n",
    "    x_max, y_max = min(x_max, rotated.shape[1]), min(y_max, rotated.shape[0])\n",
    "    \n",
    "    return rotated[y_min:y_max, x_min:x_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "cf3d5ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_tag(image, tag_path='tag.png'):\n",
    "    \"\"\"\n",
    "    Overlay a randomly augmented tag on the input image with transparency and 3D-like effect\n",
    "    \"\"\"\n",
    "    img = image.copy()\n",
    "    \n",
    "    # Rotate if too tall\n",
    "    if img.shape[0] / img.shape[1] <= 0.7:\n",
    "        img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "    \n",
    "    # Load the tag image\n",
    "    tag_img = cv2.imread(tag_path, cv2.IMREAD_UNCHANGED)  # supports transparency\n",
    "    if tag_img is None:\n",
    "        raise FileNotFoundError(f\"{tag_path} not found.\")\n",
    "    \n",
    "    # Ensure tag has alpha channel\n",
    "    if tag_img.shape[2] == 3:\n",
    "        b, g, r = cv2.split(tag_img)\n",
    "        alpha = np.ones(b.shape, dtype=b.dtype) * 255\n",
    "        tag_img = cv2.merge([b, g, r, alpha])\n",
    "    \n",
    "    # Apply blur\n",
    "    blur_strength = (5, 5)\n",
    "    tag_img[:, :, :3] = cv2.GaussianBlur(tag_img[:, :, :3], blur_strength, 0)\n",
    "    \n",
    "    # Subtle random darkening\n",
    "    alpha_scale = random.uniform(0.85, 0.95)\n",
    "    tag_img[:, :, :3] = (tag_img[:, :, :3] * alpha_scale).astype(np.uint8)\n",
    "    \n",
    "    # Resize tag\n",
    "    scale_factor = random.uniform(0.08, 0.11)\n",
    "    w = max(1, int(img.shape[1] * scale_factor))\n",
    "    h = max(1, int(w * 0.6))\n",
    "    tag_img = cv2.resize(tag_img, (w, h), interpolation=cv2.INTER_AREA)\n",
    "    \n",
    "    # 2D rotation with transparency\n",
    "    rotate_val = random.choice([-20, -10, 0, 10, 20])\n",
    "    tag_pil = Image.fromarray(tag_img)\n",
    "    tag_rotated = tag_pil.rotate(rotate_val, expand=True, fillcolor=(0,0,0,0))  # transparent\n",
    "    tag_img = np.array(tag_rotated)\n",
    "    \n",
    "    # 3D-like perspective\n",
    "    h_t, w_t = tag_img.shape[:2]\n",
    "    max_shift = w_t * 0.1\n",
    "    pts1 = np.float32([[0,0],[w_t,0],[w_t,h_t],[0,h_t]])\n",
    "    pts2 = np.float32([\n",
    "        [random.uniform(0, max_shift), random.uniform(0, max_shift)],\n",
    "        [w_t - random.uniform(0, max_shift), random.uniform(0, max_shift)],\n",
    "        [w_t - random.uniform(0, max_shift), h_t - random.uniform(0, max_shift)],\n",
    "        [random.uniform(0, max_shift), h_t - random.uniform(0, max_shift)]\n",
    "    ])\n",
    "    M = cv2.getPerspectiveTransform(pts1, pts2)\n",
    "    tag_img = cv2.warpPerspective(tag_img, M, (w_t, h_t), borderMode=cv2.BORDER_CONSTANT, borderValue=(0,0,0,0))\n",
    "    \n",
    "    # Random position\n",
    "    height, width = img.shape[:2]\n",
    "    x_min, x_max = width // 4, (width // 4) * 2\n",
    "    y_min, y_max = height // 4, (height // 4) * 2\n",
    "    x_rand = random.randint(x_min, int(x_max * 0.8))\n",
    "    y_rand = random.randint(y_min, int(y_max * 0.8))\n",
    "    \n",
    "    # Overlay respecting alpha channel\n",
    "    h_tag, w_tag = tag_img.shape[:2]\n",
    "    end_y = min(y_rand + h_tag, height)\n",
    "    end_x = min(x_rand + w_tag, width)\n",
    "    overlay_h, overlay_w = end_y - y_rand, end_x - x_rand\n",
    "    if overlay_h <=0 or overlay_w <=0:\n",
    "        return img  # nothing to overlay\n",
    "\n",
    "    overlay = tag_img[:overlay_h, :overlay_w]\n",
    "    alpha = overlay[:, :, 3:] / 255.0\n",
    "    img[y_rand:end_y, x_rand:end_x, :3] = (alpha * overlay[:, :, :3] + (1-alpha) * img[y_rand:end_y, x_rand:end_x, :3]).astype(np.uint8)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "a030fe96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress(image, path):\n",
    "    \"\"\"\n",
    "    Compress and degrade an image to simulate poor quality.\n",
    "    Combines low JPEG quality, slight blur, and noise.\n",
    "    \"\"\"\n",
    "    # Step 1: Save as low-quality JPEG first\n",
    "    quality = random.randint(10, 40)  # aggressive low-quality\n",
    "    img_pil = Image.fromarray(image)\n",
    "    img_pil.save(path, \"JPEG\", quality=quality, optimize=True)\n",
    "    \n",
    "    # Step 2: Reload image and apply slight blur\n",
    "    img = np.array(Image.open(path))\n",
    "    blur_size = random.choice([3,5])\n",
    "    img = cv2.GaussianBlur(img, (blur_size, blur_size), 0)\n",
    "    \n",
    "    # Step 3: Add small random noise\n",
    "    noise = np.random.normal(0, random.randint(5, 15), img.shape).astype(np.float32)\n",
    "    img = np.clip(img + noise, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    # Step 4: Save again\n",
    "    Image.fromarray(img).save(path, \"JPEG\", quality=quality, optimize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b68a9ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_images(dataset_splits: list[list[str]], base_filename: str):\n",
    "    \"\"\"\n",
    "    Crop objects from images using annotations, apply augmentation, \n",
    "    and save positive and negative samples for train/test/val splits.\n",
    "    \"\"\"\n",
    "    global_counter = 0\n",
    "\n",
    "    print(f\"\\nProcessing dataset for '{base_filename}'...\")\n",
    "\n",
    "    split_names = ['train', 'test', 'val']\n",
    "    for split_idx, image_paths in enumerate(dataset_splits):\n",
    "        split_name = split_names[split_idx]\n",
    "        print(f\"Generating {split_name} set with {len(image_paths)} images...\")\n",
    "\n",
    "        # Ensure directories exist once per split\n",
    "        for label in ['0', '1']:\n",
    "            Path(f\"processed_dataset/{split_name}/{label}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        for img_idx, img_path in enumerate(tqdm(image_paths, desc=f\"Processing {split_name}\", ncols=100)):\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is None:\n",
    "                print(f\"  Could not read image: {img_path}\")\n",
    "                continue\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Load annotation\n",
    "            ann_path = Path(img_path).with_suffix('.txt')\n",
    "            if not ann_path.exists():\n",
    "                print(f\"  Missing annotation file: {ann_path}\")\n",
    "                continue\n",
    "\n",
    "            # Read bounding boxes\n",
    "            bboxes = []\n",
    "            with open(ann_path, \"r\") as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 5:\n",
    "                        continue  # skip malformed lines\n",
    "                    # Skip class ID, keep only bbox coordinates\n",
    "                    bboxes.append([float(x) for x in parts[1:]])\n",
    "\n",
    "            if not bboxes:\n",
    "                print(f\"  No bounding boxes in: {ann_path}\")\n",
    "                continue\n",
    "\n",
    "            # Crop and augment each object\n",
    "            for obj_idx, bbox in enumerate(bboxes):\n",
    "                cropped_obj = crop_rotated_bbox(img, *bbox)\n",
    "\n",
    "                # Generate augmented negative version\n",
    "                neg_obj = overlay_tag(cropped_obj)\n",
    "\n",
    "                # Generate unique filename\n",
    "                filename = f\"{base_filename}_{obj_idx}_{global_counter}.jpg\"\n",
    "\n",
    "                pos_path = os.path.join(\"processed_dataset\", split_name, \"0\", filename)\n",
    "                neg_path = os.path.join(\"processed_dataset\", split_name, \"1\", filename)\n",
    "\n",
    "                # Save images\n",
    "                compress(cropped_obj, pos_path)\n",
    "                compress(neg_obj, neg_path)\n",
    "\n",
    "                global_counter += 1\n",
    "\n",
    "        print(f\"  ✅ Completed {split_name} set for '{base_filename}' ({global_counter} images total so far)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a82e769f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    \"\"\"\n",
    "    Splits data into train, test and val\n",
    "    \"\"\"\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=42, shuffle = False)\n",
    "    train, val = train_test_split(train, test_size=0.2, random_state=42, shuffle = False)\n",
    "    \n",
    "    return [train, test, val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "70fbf4d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing dataset for 'Meeting1'...\n",
      "Generating train set with 716 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|███████████████████████████████████████████| 716/716 [04:23<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed train set for 'Meeting1' (1987 images total so far)\n",
      "Generating test set with 224 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|████████████████████████████████████████████| 224/224 [01:42<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed test set for 'Meeting1' (2659 images total so far)\n",
      "Generating val set with 179 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|█████████████████████████████████████████████| 179/179 [01:27<00:00,  2.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed val set for 'Meeting1' (3196 images total so far)\n",
      "\n",
      "Processing dataset for 'Meeting2'...\n",
      "Generating train set with 716 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|███████████████████████████████████████████| 716/716 [04:53<00:00,  2.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed train set for 'Meeting2' (2003 images total so far)\n",
      "Generating test set with 225 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test: 100%|████████████████████████████████████████████| 225/225 [01:30<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed test set for 'Meeting2' (2601 images total so far)\n",
      "Generating val set with 180 images...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|█████████████████████████████████████████████| 180/180 [01:18<00:00,  2.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ✅ Completed val set for 'Meeting2' (3141 images total so far)\n",
      "Completed processing the dataset!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "HABBOF_path = 'HABBOF'\n",
    "# folders = [\"Lab1\", \"Lab2\", \"Meeting1\", \"Meeting2\"]\n",
    "folders = [\"Meeting1\", \"Meeting2\"]\n",
    "\n",
    "for folder in folders:\n",
    "    folder_path = os.path.join(HABBOF_path, folder)\n",
    "    \n",
    "    # Get all jpg files in the folder\n",
    "    data_arr = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.jpg')]\n",
    "    \n",
    "    # Generate dataset images\n",
    "    generate_dataset_images(split_dataset(data_arr), folder)\n",
    "\n",
    "print(\"Completed processing the dataset!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
